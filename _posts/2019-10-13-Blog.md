---
layout: post
title: "Maximal Marginal Relevance to Re-rank results in KeyPhrase Extraction"
categories:
  - Research Paper, NLP
tags:
  - Machine learning, NLP
excerpt_separator:  <!--more-->
last_modified_at: 
---

Maximal Marginal Relevance a.k.a. MMR has been introduced in this paper [The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf). MMR tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc.<!--more-->

Let's try to understand the above scenario by taking an example and will see how MMR is helpful in that case.

I was recently working on KeyPhrase extraction from a set of documents which belongs to one category. I have used different approaches (TextRank, RAKE, POS tagging etc.. to name a few) to extract keywords from the documents along with some score. Now that score is used as ranking of the phrases for that document. 

But now here comes some issue with this approach, lets say your keyPhrases are like `(Good Product, Great Product, Nice Product, Excellent Product, Easy Install, Nice UI, Light weight etc)` , then all the phrases like `good product, nice product, excellent product` are similar and basically define same property of product. Suppose we have a space to show just 5 keyPhrases, in that case, we don't want to show all these similar phrases.

![KeyPhrase Extraction](https://github.com/aditya00kumar/aditya00kumar.github.io/blob/master/_screenshots/Keyphrase.png)

We want to properly utilize this space for 5 keyPhrases so that the information displayed by the Keyphrases about the documents is diverse enough such that only one type of phrases do not dominate in the whole space.

So how we can do that, there might be variety of approaches to solve this issue. For sake of simplicity and completness of the article, I am going to discuss two approaches:

1. **Remove redundant phrases using cosine similarity**

   This is the naive approach that came to mind to deal with similar kind of terms. Just use word embeddings and find embeddings of phrases and find cosine similarity between embeddings and just set a threshold above which you will consider the terms as similar.


    ```python
    from sklearn.metrics.pairwise import cosine_similarity
    def club_similar_keywords(emb_mat, sim_score=0.9):
       """
       :param emb_mat: matrix having vectors with words as index
       :param sim_score: 0.9 by default
       :return: returns list of unique words from index after combining words which has similarity score of more than
       0.9
       """
       if len(emb_mat) == 0:
           return 'NA'
       xx = cosine_similarity(emb_mat)
       final_keywords = set(emb_mat.index)
       N = len(emb_mat.index)
       dd = {}
       for i in range(N):
           for j in range(N):
               if (float(xx[i][j]) > sim_score) and (i != j):
                   try:
                       dd[emb_mat.index[i]].append(emb_mat.index[j])
                   except:
                       dd[emb_mat.index[i]] = []
                       dd[emb_mat.index[i]].append(emb_mat.index[j])
       removed_keywords = []
       for key in dd:
           for val in dd[key]:
               if key not in removed_keywords:
                   removed_keywords += dd[key]
                   try:
                       final_keywords.remove(val)
                   except:
                       pass
       return final_keywords
    ```

   An issue with this approach is that you need to set the threshold above which, terms will be clubbed together. And sometimes very close keywords also have cosine similarity < 0.8 where glove word embeddings have been used to convert the sentence to vector by averaging word tokens.

2. **Re-Ranking the KeyPhrases using MMR**

   <img src="https://latex.codecogs.com/png.latex?\inline&space;$MMR&space;=&space;\operatorname*{Arg\,max}_{D_i&space;\in&space;{R/S}}[\lambda&space;(Sim_1(D_i,&space;Q)-&space;(1-\lambda)&space;{\max}_{D_i&space;\in&space;S}&space;Sim_2(D_i,&space;D_j))]$" title="$MMR = \operatorname*{Arg\,max}_{D_i \in {R/S}}[\lambda (Sim_1(D_i, Q)- (1-\lambda) {\max}_{D_i \in S} Sim_2(D_i, D_j))]$" />
   
   where, 
        Q = Query (Description of Document category)<br>
   		D = Set of documents related to Query Q <br>
   		S = Subset of documents in R already selected <br>
   		R\S = set of unselected documents in R <br>
   		$\lambda$ = Constant in range [0 - 1], for diversification of results

   In the below implementation of MMR, cosine similarity has been considered as $Sim_1$ and $Sim_2$.

   Any other similarity measure like lexrank can be taken and modify the function accordingly.

```python
from sklearn.metrics.pairwise import cosine_similarity
def maximal_marginal_relevance(sentence_vector, phrases, embedding_matrix, lambda_constant=0.5, threshold_terms=10):
    """
    Return ranked phrases using MMR. Cosine similarity is used as similarity measure.
    :param sentence_vector: Query vector
    :param phrases: list of candidate phrases
    :param embedding_matrix: matrix having index as phrases and values as vector
    :param lambda_constant: 0.5 to balance diversity and accuracy. if lambda_constant is high ,      then higher accuracy. If lambda_constant is low then high diversity.
    :param threshold_terms: number of terms to include in result set
    :return: Ranked phrases with score
    """
    # todo: Use cosine similarity matrix for lookup among phrases instead of making call everytime.
    s = []
    r = sorted(phrases, key=lambda x: x[1], reverse=True)
    r = [i[0] for i in r]
    while len(r) > 0:
        score = 0
        phrase_to_add = ''
        for i in r:
            first_part = cosine_similarity([sentence_vector], [embedding_matrix.loc[i]])[0][0]
            second_part = 0
            for j in s:
                cos_sim = cosine_similarity([embedding_matrix.loc[i]], [embedding_matrix.loc[j[0]]])[0][0]
                if cos_sim > second_part:
                    second_part = cos_sim
            equation_score = lambda_constant*(first_part-(1-lambda_constant) * second_part)
            if equation_score > score:
                score = first_part - (1 - lambda_constant) * second_part
                phrase_to_add = i
        if phrase_to_add == '':
            phrase_to_add = i
        r.remove(phrase_to_add)
        s.append((phrase_to_add, score))
    return (s, s[:threshold_terms])[threshold_terms > len(s)]
```

Setting $ \lambda $ to 0.5 gives the optimal mix of diversity and accuracy in result set.

Now on using MMR, similar results are ranked far away. So the issue to select top 5 keyPhrase has been resolved as all similar terms are not grouped  and don't appear in the final result.

**References:**

1. [The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)
2. [Simple Unsupervised Keyphrase Extraction using Sentence Embeddings](https://arxiv.org/pdf/1801.04470.pdf)
