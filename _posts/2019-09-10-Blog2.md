---
layout: post
title: "Deep Averaging Network in Universal Sentence Encoder"
categories:
  - Blog post
tags:
  - NLP
excerpt_separator:  <!--more-->
last_modified_at: 2013-03-09T14:25:52-05:00
---
<div style="text-align: justify">Word embeddings are now state of art for doing downstream NLP tasks such as text classification, sentiment analysis, sentence similarity etc and provides very good results compared to tf-idf or count vectorizer. Using word embeddings we can find the similarity between words and can apply vector operations and therefore can easily distinguish between `cat, dog, car`. Here `cat and dog` will be more similar compared to `car`.</div>

<!--more-->
But obtaining vectors for sentences is not immediate obvious. This post tries to explain one of the approaches described in [**Universal Sentence Encoder**](https://arxiv.org/pdf/1803.11175.pdf).

Deep averaging network (DAN): Idea of DAN is described in this paper [**Deep Unordered Composition Rivals Syntactic Methods for Text Classification**](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)

Word  embeddings are low dimensional vector in N dimensional space which  describe a word. To obtain vector space model for sentences or  documents, appropriate composition function is required. Composition  function is mathematical process of combining multiple words into single  vector.

Composition functions are of two types

1. **Unordered:** Treats as bag of word embeddings
2. **Syntactic:** Takes word order and sentence structure into account. Syntactic  functions outperform unordered functions on many tasks but at same time  it is compute expensive and requires more training time.

Deep  unordered model that obtains near state of art accuracy on sentence and  document level tasks with very less training time works in three steps:
- take the vector average of the embeddings associated with an input sequence of tokens
- pass that average through one or more feed-forward layer
- perform (linear) classification on the final layers representation
- Loss function is cross entropy.

